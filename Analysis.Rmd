---
title: "Pageview-Analysis"
output: html_document
---

Goal: test if the frontend had a significant impact on pageviews. Idea: use Wikipedia-pageviews on school-materials to predict Serlo-ones (and account therefore for some temporal differences) and introduce frontend as additional predictor whose influence and significance we want to calculate/test.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("magrittr")
```

# Intro

We have two datasets: the variance and mean-normalised data `normalised_data` and the non-normalised original data `non_normalised`. Reading both in:
```{r}
#setwd("~/Desktop/Serlo/Aufrufe_Analyse")
normalised_data <- read.csv("pageviews.csv", colClasses=c("Date",NA, NA))
normalised_data <- as_tibble(normalised_data)

non_normalised <- read.csv("pageviews-not-normalized.csv",colClasses=c("Date",NA, NA))
non_normalised <- as_tibble(non_normalised)

head(normalised_data)
head(non_normalised)

```

I introduce two new variables: the full-deployment of the frontend, starting from the 28.01.21 and the beginning of Germany-wide homeschooling on the 15.12.2020: 

```{r}
normalised_data %<>% mutate(frontend = ifelse(timestamp >= as.Date("2021-01-28"), TRUE, FALSE))
normalised_data %<>% mutate(homeschooling = ifelse(timestamp >= as.Date("2020-12-15"), TRUE, FALSE))
normalised_data %<>% drop_na()

non_normalised %<>% mutate(frontend = ifelse(timestamp >= as.Date("2021-01-28"), TRUE, FALSE))
non_normalised %<>% mutate(homeschooling = ifelse(timestamp >= as.Date("2020-12-15"), TRUE, FALSE))
non_normalised %<>% drop_na()

```

Plots showing the joint variation of both page-visits (plotting the normalised values here):
```{r}
normalised_data %>%
  ggplot(aes(timestamp)) +
  geom_line(aes(y=serlo, col="red")) +
  geom_line(aes(y=wikipedia, col="blue")) +
  ggtitle("Serlo (red) vs Wikipedia-Calls (blue)")

```

I want to use the Wikipedia-Calls to predict the Serlo-ones. Plots of all data-points split up by homeschooling:

```{r}
non_normalised %>% 
  ggplot(aes(x=wikipedia, y=serlo, col=homeschooling)) +
  geom_point()

```

Interesting is the rrelatively strong linearity when homeschooling is there, but the higher variance otherwise. It may be an effect of not enough datapoints or something more fundamental: 
```{r}
non_normalised %>% 
  filter(homeschooling==TRUE) %>%
  ggplot(aes(x=wikipedia, y=serlo)) +
  geom_point() + 
  ggtitle("Linear prediction when homeschooling is active")

non_normalised %>% 
  filter(homeschooling==FALSE) %>%
  ggplot(aes(x=wikipedia, y=serlo)) +
  geom_point() +
  ggtitle("Linear prediction without homeschooling")

normalised_data %>%
  ggplot(aes(x=wikipedia, y=serlo)) +
  geom_point() +
  ggtitle("All datapoints")

non_normalised %>% group_by(homeschooling) %>% count(n=n())
```

Given `homeschooling=TRUE`, it seems like we have a nearly linear trend, with still some deviations for big and small values. For the other datapoints there it more deviation in the middle/upper end of the graph --> it seems like the error-variance is depending on the predictor-values.

# Linear models

Looking at these plots we see that fitting a linear model is probably problematic, since probably the normality-assumption for the residuals does not hold. This is true: 

```{r}
fit <- lm(serlo ~ wikipedia + homeschooling + frontend, normalised_data)
summary(fit)
plot(fit)
```

And a formal ks-test on normality of the residuals lets us to strongly reject their normality:

```{r}
ks.test(resid(fit), "pnorm")

```

Here I introduced `homeschooling` as additional predictor, since it seems to slightly change aboves relationship between Serlo and Wikipedia-pageviews. 

We need to think of another model.  Given `homeschooling=FALSE` it seems like the error-variance in a linear-model we fit, depends on the pageviews of our Wikipedia-random-variable $W$. I can try a multiplicative model: 

$$ S \sim W\epsilon \implies log(S) \sim log(W) + log(\epsilon) \text{ with } \epsilon \sim N(1, \sigma^2)$$

Trying a log-transformation on both-sides, but still with Gaussian Errors I get: 

```{r}
fit_log_trafo <- lm(log(serlo) ~ log(wikipedia) + homeschooling, non_normalised)
summary(fit_log_trafo)
plot(fit_log_trafo)
```

And without homeschooling-predictor:
```{r}
fit_log_trafo_wout_homeschooling <- lm(log(serlo) ~ log(wikipedia), non_normalised)
summary(fit_log_trafo_wout_homeschooling)
plot(fit_log_trafo_wout_homeschooling)
```

This already seems better (quite good even),  without it being it. Now I can try a the above model with log-normal-errors. 

A bit googling around I found this [thread]()
https://stats.stackexchange.com/questions/47840/linear-model-with-log-transformed-response-vs-generalized-linear-model-with-log) and the associated Paper. It seems to be in general better to fit a glm then with Gaussian family and log as link, than a linear model with lognormal-error. Doing that gives (first only looking at the non-homeschooling-value): 

```{r}
hs_false <- non_normalised %>% filter(homeschooling==FALSE)

glm_log_Gaussian_hs_false <- glm(serlo ~ wikipedia , family=gaussian(link="log"), hs_false)
summary(glm_log_Gaussian_hs_false)
```

A small plot: 

```{r}
wikipedia_vals <- seq(min(hs_false$wikipedia), max(hs_false$wikipedia), 1)
y_vals <- predict(glm_log_Gaussian_hs_false, list(wikipedia = wikipedia_vals), type="response")

plot(hs_false$wikipedia, hs_false$serlo)
lines(wikipedia_vals, y_vals)
```

I need to read into sanity-checks for glms, but it seems quite okay at first glance. This could be a model we could expand on. Now dropping the `homeschooling=FALSE`-condition and including as additional predictor in this glm

```{r}
glm_log_Gaussian <- glm(serlo ~ wikipedia +homeschooling , family=gaussian(link="log"), non_normalised)
summary(glm_log_Gaussian)


```
This is also okay, but doesn't add much. We could now try to expand on this model or use the empirical finding of a stronger linear relationship between Serlo and Wikipedia, given `homeschooling=TRUE`, so fit a piecewise-model:

\[S = \begin{cases} 
      \alpha W + \beta + \epsilon & homeschooling = TRUE \\
      \gamma W\epsilon & homeschooling = FALSE
      \end{cases}
\]

The influence on homeschooling on the values is something we need to research on more. Just for illustration: linear model given `homeschooling=TRUE`:
```{r}
hs_true <- non_normalised %>% filter(homeschooling==TRUE)
fit_hs_true <- lm(serlo ~ wikipedia, hs_true)
summary(fit_hs_true)
plot(fit_hs_true)

```

And values vs the line:
```{r}
plot(hs_true$wikipedia, hs_true$serlo)
lines(hs_true$wikipedia, predict(fit_hs_true))
```

We still have considerable deviations from normality, otherwise this seems a solid predictor --> useful for prediction, but not necessarily for interpretation since.

# GAM

Overall we see that a linear model is not really appropriate and also the glm has a very high deviance. I now try a non-parametric-method with a gam, meaning I fit a model:

$$\mathbb E[S~|~\mathbf x] \equiv \alpha + \sum_{j = 1}^p f_j(x_j),$$
with our predictors $\mathbf x$ as splines or linear inputs. Having homeschooling as additive input and using a spline for wikipedia I get:

```{r}
library(gam)
normalised_gam <- gam(serlo ~ s(wikipedia) + homeschooling + frontend, data = normalised_data)

plot(normalised_gam, se = TRUE)

summary(normalised_gam)
```

This seems to fit really well.  We see from the ANOVA for nonparametric effects, that there is slight evidence for rejecting $H_0$ that the Wikipedia-contribution is linear $f_W$, what we also saw above and making these approaches slightly problematic. We also see that frontend has no significant effect, whilst homeschooling and Wikipedia have a big one (with still Wikipedia dominating everything and homeschooling providing a slight adjustment on the level). One interpretation for this effect is that teachers are more likely to recommend Serlo in homeschooling that Wikipedia, shifting everything slightly

--> This model is really interesting and could give us a lot of info. I think we can now iterate on that and add predictors/expand.

# Model Comparison

Now we can compare the models. We have already seen above that the GAM-model does best when it comes to interpretation, since the assumptions of the linear models are slightly flawed and the GLM has really high deviance. But what about predictive power?

I do a 100 splits in training and test-data and and calculate the mean l1-error over those. For benchmarking-purposes I also include fitting a randomForest, which I did not include above, due to the difficult interpretability of it. I get:

```{r}
library(randomForest)
n <- nrow(non_normalised)
train_size <- round(2/3 * n)
folds <- 100

gam_error <- numeric(folds)
glm_error <- numeric(folds)
log_trafo_error <- numeric(folds)
linear_error <- numeric(folds)
rndforest_error <- numeric(folds)

for(i in 1:folds){
  if(i%%10 == 0) cat("Calculating fold ", i, "\n")
  train_points <- sample(n, train_size)
  train_set <- non_normalised[train_points, ]
  test_set <- non_normalised[-train_points, ]
  
  gam_model <- gam(serlo ~ s(wikipedia) + homeschooling + frontend, data = train_set)
  gam_error[i] <- mean(abs(predict(gam_model, test_set) - test_set$serlo))
  
  glm_log_Gaussian_model <- glm(serlo ~ wikipedia +homeschooling , family=gaussian(link="log"), data = train_set)
  glm_error[i] <- mean(abs(predict(glm_log_Gaussian_model, test_set) - test_set$serlo))
  
  log_trafo_model <- lm(log(serlo) ~ log(wikipedia) + homeschooling, data = train_set)
  log_trafo_error[i] <- mean(abs(predict(log_trafo_model, test_set) - test_set$serlo))
  
  linear_model <- lm(serlo ~ wikipedia + homeschooling + frontend, data = train_set)
  linear_error[i] <- mean(abs(predict(linear_model, test_set) - test_set$serlo))
  
  rndforest_model <- randomForest(serlo ~ wikipedia + homeschooling + frontend, data = train_set)
  rndforest_error[i] <- mean(abs(predict(rndforest_model, test_set) - test_set$serlo))

}

mean_errors <- data.frame(names=c("GAM", "GLM", "Linear with log-trafo", "Linear model", "Random Forest"), mean_error_value=c(mean(gam_error), mean(glm_error), mean(log_trafo_error), mean(linear_error), mean(rndforest_error)))

print("Comparing the models on 100 folds of the training data and l1-error we get the following order:")
print(mean_errors[order(mean_errors$mean_error_value),])
```

So we see that the GAM-model achieves also the best performance (in all tests that I ran and even higher than the randomForest which is astonishing), besides being the one with the highest interpretability, quickly followed by the linear model, which seems to be a good predictor and approximation to the GAM, but whose assumptions are flawed as we saw above, and far before the GLM and linear model with log-trafo. More data will show, if this is also the case in future.



------------

Other notes: 

- Homeschooling absorbes quite a lot of the frontend effects so far in the flawed linear model and in the gam
- Under the condition of homeschooling we seems to have a linear trend (exception big and small values) --> why is that? Can we work with that? 
- Including a homeschooling-wikipedia-interaction is really interesting:

```{r}
summary(lm(serlo ~ wikipedia*homeschooling + frontend, normalised_data))
```

The interaction is quite strong and significant. Of course the $R^2$ is higher (more predictors), but also the AIC is way lower: 

```{r}
cat("AIC without interaction: ", AIC(fit))
cat("AIC with interaction: ", AIC(lm(serlo ~ wikipedia*homeschooling + frontend, normalised_data)))
```

Is it useful to integrate the interaction? What would be an interpretation and isn't it faulty? It seems a bit like data dredging to find some significant interaction, but maybe there is a real interpretation (is there, since homeschooling only has two levels this amounts to $\alpha W + \beta I_{hs} W + \gamma I_{hs}$, meaning that homeschooling does not only change the level (like a new mean-term, but also the influence of wikipedia --> maybe this is right: teachers get pupils onto serlo) .

- We could experiment with different glms --> Poisson-regression
- The influence of frontend is not a step-function, but rises linearly or exponentially over time. Model like $S_t = \alpha W_t + \beta e^t I_{f} + \dots$. --> exponential weighting problematic, but sigmoid-weighting, or relu?


